#+TITLE: Information theory. Homework 1.
#+AUTHOR: Dmitry Mukhutdinov, M3439

* Abstract
  The task was to estimate entropy per char for given file, in order to estimate
  potential compression efficiency. We do that by calculating empirical
  probability distributions for words of various lenghts - 1, 2, 3, 4.
  Afterwards we compare potential compression efficiency with one reached by
  real compression algorithm.

  My variant is 15, so I got the file ~alice29.txt~ to investigate.
  
  DISCLAIMER: The actual size of file ~alice29.txt~ in archive ~canterbury.zip~, which
  is shared in Google Drive folder
  ([[https://drive.google.com/drive/folders/0B2_h6Owy5rgydTNTaU5VWjljTDQ]]) differs
  from the size given in ~list_of_files.docx~

  Declared size: 152,089 bytes
  Actual size: 148,481 bytes

  Therefore, if correct answers (to which my answers are compared) are
  calculated with file of size 152,089 bytes, then this file must differ from
  file provided in the archive. If that's the case, then please, recount the
  correct answers using your software on the file which is actually present in
  the archive. For convenience, I attach this file to this email.

  The code for accomplishing the task is written in Haskell. Snippets from the
  code are given below with comments.
* Main task
  To count empyrical probability of $n$-length word, we first count numbers of
  entries in text for every $n$-length word, and after that we divide it by
  total number of $n$-length substrings in text.

  Code:
  #+BEGIN_SRC haskell
   slice :: Int64 -> Int64 -> ByteString -> ByteString
   slice start len = B.take len . B.drop start

   wordProbas :: WordSize -> ByteString -> WordProbas
   wordProbas n text = fmap calcProba $ foldr countWord H.empty [0..blocksCount]
     where blocksCount = B.length text - n
           calcProba x = fromIntegral x / fromIntegral blocksCount
           countWord offset = H.alter incOr1 $ slice offset n text
           incOr1 val = fmap (+1) val <|> Just 1
  #+END_SRC
   
  Here function ~wordProbas~ takes length of the word ~n~ and text itself
  ~text~, and returns hashmap, where words are the keys and probabilities are
  the values.

  Given the probabilities of each n-length word present in the text, we can
  count n-dimensional entropy:
  #+BEGIN_SRC haskell
   entropy :: WordSize -> ByteString -> Double
   entropy n text = - sum summands / fromIntegral n
   where wordCount = fromIntegral $ B.length text - n
         summands = map (\x -> x * logBase 2 x) probas
         probas = H.elems $ wordProbas n text
  #+END_SRC
   
  This function is using ~wordProbas~ to get probability distribution on words,
  and using this distribution it calculates mathematical expectation of
  self-information over set of words (which is entropy).
   
  We get the following results (for $n \in {1, 2, 3, 4}$):
  | $n$ | $H_n(X)$  | possible compression rate |
  |-----+-----------+---------------------------|
  |   1 | 4.5128975 |                  1.772697 |
  |   2 | 4.0073148 |                  1.996349 |
  |   3 | 3.5084407 |                  2.280215 |
  |   4 | 3.0801469 |                  2.597279 |

  Possible compression rate here is calculated as $8 / H_n(X)$, because in
  uncompressed file one char is 8 bits, and since entropy is mathematical expectation of
  self-information, it shows how much bits we need for one char in average if
  we want to encode this text. 

  Then we compress the file with GZip. The size of compressed file is 53,647
  bytes (vs 148,481 bytes of uncompressed file), which give us compression
  ratio of 2.76774. This is more than any of theoretical compression rates we
  calculated. Why?

  Actually, this makes perfect sense: our file is English text. GZip would
  definitely detect repeated words, and would try to map them onto some smaller
  alphabet. Considering words only 4 chars long is simply not enough there,
  because average length of English word is 5 chars.
   
  To prove the hypothesis, let's count the entropy and possible compression
  rate for $n = 5$.
  | $n$ | $H_n(X)$ | possible compression rate |
  |-----+----------+---------------------------|
  |   5 |  2.72829 |                 2.9322395 |

  So for $n = 5$ theoretical compression rate is indeed better then practical.

* Additional task 
  Given that first half of the file is already known to decoder, we need to come
  up with encoding strategy for the second half. It's also given that for every
  word $a$ we know $p(a)$, we have a way to encode this char with exactly $log
  p(a)$ bits.

  The simple strategy would be to encode all the words in the second half
  using probabilities of these words (and corresponding encodings) from the
  first half. To reason about efficiency of such method, let's calculate
  entropies for first and second file halves separately:

  | $n$ | first half | second half |
  |-----+------------+-------------|
  |   1 |    4.47028 |     4.54745 |
  |   2 |    3.98167 |     4.00867 |
  |   3 |    3.48229 |     3.47758 |
  |   4 |    3.04195 |     3.02232 |
  
  As we can see, the entropy of the second half is approximately the same as
  entropy of the first half. Therefore, if we have a way to encode words from
  the first half perfectly, it wouldn't be excessive for encoding the second
  half, and such encoding would be efficient.
